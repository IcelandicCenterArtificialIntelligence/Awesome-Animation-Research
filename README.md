![img1](https://github.com/zhenglinpan/Awesome-Animation-Research/blob/master/assets/teaser.gif)

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/amirhossein-kz/Awesome-Diffusion-Models-in-Medical-Imaging) 
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![Teaser](https://img.shields.io/badge/Teaser_by-ÂØùÂõΩ-pink)](https://space.bilibili.com/177312952?spm_id_from=333.337.0.0)
<p align="left">
   ENGLISH | <a href="https://github.com/zhenglinpan/Awesome-Animation-Research/blob/master/README_CN.md">ÁÆÄ‰Ωì‰∏≠Êñá</a>
</p>

# Awesome Animation Research üé•üìö

This repository provides a curated collection of dataset, research, and resources related to **cel animation(cartoon video)** specifically. 

**What You'll Find Here:** Papers/Dataset/Repo closely related to cel animation(cartoon video) that could potentially assist creating animation. e.g. Inbetweening, Genga Colorization. 

**What's Not Included:** General Anime Research. i.e. Anime Style Transfer, Anime Image Enhancement, Anime Image generation. If you are interested in general anime research, please refer to [AwesomeAnimeResearch](https://github.com/SerialLain3170/AwesomeAnimeResearch).

****

Creating animation is time-consuming and often involves manual work. AI tools are changing this landscape. Researchers are tackling animation-specific challenges like inbetweening and frame-to-frame color propagation. 

You might notice the repo's list is currently short. Animation research is a relatively new and niche area, and we look forward to more researchers, including you, contributing to its growth.

The repo will keep track of the latest research. Feel free to follow and star ! üåü

## New Papers
<!-- [<span style="color:red">*new</span>]  -->

üö©„ÄêInbetween„Äë **Joint Stroke Tracing and Correspondence for 2D Animation** \
*Haoran Mo, Chengying Gao, Ruomei Wang*\
[9 Apr., 2024] [SIGGRAPH, 2024] \
[[paper](https://dl.acm.org/doi/10.1145/3649890)] | [[webpage](https://markmohr.github.io/JoSTC/)] | [demo] | [[repo](https://github.com/MarkMoHR/JoSTC)] | [dataset]

üö©„ÄêColorization„Äë **Learning Inclusion Matching for Animation Paint Bucket Colorization** \
*Yuekun Dai, Shangchen Zhou, Qinyue Li, Chongyi Li, Chen Change Loy*\
[2024] [CVPR, 2024] \
[[paper](https://arxiv.org/abs/2403.18342)] | [[webpage](https://ykdai.github.io/projects/InclusionMatching)] | [[demo](https://www.youtube.com/watch?v=nNnPUItGvSo&ab_channel=YuekunDai)] | [[repo](https://github.com/ykdai/BasicPBC)] | [[dataset](https://github.com/ykdai/BasicPBC/tree/main/dataset)]

üö© „ÄêEditing„Äë **Re:Draw -- Context Aware Translation as a Controllable Method for Artistic Production** \
*Joao Liborio Cardoso, Francesco Banterle, Paolo Cignoni, Michael Wimmer*\
[Jan., 2024] [*TBA 2024] \
[[paper](https://arxiv.org/abs/2401.03499)] | [webpage] | [demo] | [repo] | [dataset]



## Datasets

**AnimeRun: 2D Animation Visual Correspondence from Open Source 3D Movies** \
*Li Siyao, Yuhang Li, Bo Li, Chao Dong, Ziwei Liu, Chen Change Loy* \
[10 Nov., 2022] [arXiv, 2022] \
[[paper](https://arxiv.org/abs/2211.05709)] | [[webpage](https://lisiyao21.github.io/projects/AnimeRun)] | [demo] | [[repo](https://github.com/lisiyao21/AnimeRun)] | [[dataset](https://lisiyao21.github.io/projects/AnimeRun)]



## Colorization

**Learning Inclusion Matching for Animation Paint Bucket Colorization** \
*Yuekun Dai, Shangchen Zhou, Qinyue Li, Chongyi Li, Chen Change Loy*\
[2024] [CVPR, 2024] \
[[paper](https://arxiv.org/abs/2403.18342)] | [[webpage](https://ykdai.github.io/projects/InclusionMatching)] | [[demo](https://www.youtube.com/watch?v=nNnPUItGvSo&ab_channel=YuekunDai)] | [[repo](https://github.com/ykdai/BasicPBC)] | [[dataset](https://github.com/ykdai/BasicPBC/tree/main/dataset)]

**Coloring anime line art videos with transformation region enhancement network** \
*Ning Wang, Muyao Niu, Zhi Dou, Zhihui Wang, Zhiyong Wang, Zhaoyan Ming, Bin Liu, Haojie Li*\
[Sep., 2023] [Elsevier, 2023] \
[[paper](https://www.sciencedirect.com/science/article/abs/pii/S0031320323002625)] | [webpage] | [demo] | [repo] | [dataset]

**SketchBetween: Video-to-Video Synthesis for Sprite Animation via Sketches** \
*Dagmar Lukka Loftsd√≥ttir, Matthew Guzdial*\
[1 Sep., 2022] [ECCV, 2022] \
[[paper](https://arxiv.org/abs/2209.00185)] | [webpage] | [demo] | [[repo](https://github.com/ribombee/SketchBetween)] | [dataset]


**Animation Line Art Colorization Based on Optical Flow Method** \
*Yifeng Yu, Jiangbo Qian, Chong Wang, Yihong Dong, Baisong Liu*\
[27 Aug., 2022] [SSNR, 2022] \
[[paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4202289)] | [webpage] | [demo] | [repo] | [dataset]

**The Animation Transformer: Visual Correspondence via Segment Matching** \
*Evan Casey, V√≠ctor P√©rez, Zhuoru Li, Harry Teitelman, Nick Boyajian, Tim Pulver, Mike Manh, William Grisaitis*\
[6 Sep., 2021] [arXiv, 2021] \
[[paper](https://arxiv.org/abs/2109.02614)] | [webpage] | [[demo](https://cadmium.app/)] | [repo] | [dataset]

**Artist-Guided Semiautomatic Animation Colorization** \
*Harrish Thasarathan, Mehran Ebrahimi* \
[22 Jun., 2020] [arXiv, 2020] \
[[paper](https://arxiv.org/abs/2006.13717)] | [webpage] | [demo] | [repo] | [dataset]

**Line Art Correlation Matching Feature Transfer Network for Automatic Animation Colorization** \
*Zhang Qian, Wang Bo, Wen Wei, Li Hai, Liu Jun Hui* \
[14 Apr., 2020] [arXiv, 2020] \
[[paper](https://arxiv.org/abs/2004.06718)] | [webpage] | [demo] | [repo] | [dataset]

**Deep Line Art Video Colorization with a Few References** \
*Min Shi, Jia-Qi Zhang, Shu-Yu Chen, Lin Gao, Yu-Kun Lai, Fang-Lue Zhang* \
[24 Mar., 2020] [arXiv, 2020] \
[[paper](https://arxiv.org/abs/2003.10685)] | [webpage] | [demo] | [repo] | [dataset]

**Automatic Temporally Coherent Video Colorization** \
*Harrish Thasarathan, Kamyar Nazeri, Mehran Ebrahimi* \
[[paper](https://arxiv.org/abs/1904.09527)] | [webpage] | [demo] | [[repo](https://github.com/Harry-Thasarathan/TCVC)] | [dataset]



## Inbetweening & Interpolation

**Joint Stroke Tracing and Correspondence for 2D Animation** \
*Haoran Mo, Chengying Gao, Ruomei Wang*\
[9 Apr., 2024] [SIGGRAPH, 2024] \
[[paper](https://dl.acm.org/doi/10.1145/3649890)] | [[webpage](https://markmohr.github.io/JoSTC/)] | [demo] | [[repo](https://github.com/MarkMoHR/JoSTC)] | [dataset]

**Deep Geometrized Cartoon Line Inbetweening** \
*Li Siyao, Tianpei Gu, Weiye Xiao, Henghui Ding, Ziwei Liu, Chen Change Loy*\
[Nov., 2023] [ICCV 2023] \
[[paper](https://openaccess.thecvf.com/content/ICCV2023/html/Siyao_Deep_Geometrized_Cartoon_Line_Inbetweening_ICCV_2023_paper.html)] | [webpage] | [[demo](https://www.youtube.com/watch?v=iUF-LsqFKpI&ab_channel=SiyaoLi)] | [[repo](https://github.com/lisiyao21/AnimeInbet)] | [[dataset](https://drive.google.com/file/d/1SNRGajIECxNwRp6ZJ0IlY7AEl2mRm2DR/view)]


**Exploring inbetween charts with trajectory-guided sliders for cutout animation** \
*T Fukusato, A Maejima, T Igarashi, T Yotsukura*\
[1 Sep., 2023] [MTA 2023] \
[[paper](https://link.springer.com/article/10.1007/s11042-023-17354-x)]

**Enhanced Deep Animation Video Interpolation** \
*Wang Shen, Cheng Ming, Wenbo Bao, Guangtao Zhai, Li Chen, Zhiyong Gao*\
[25 Jun., 2022] [arXiv, 2022] \
[[paper](https://arxiv.org/abs/2206.12657)]

**Improving the Perceptual Quality of 2D Animation Interpolation** \
*Shuhong Chen, Matthias Zwicker*\
[24 Nov., 2021] [arXiv, 2021] \
[[paper](https://arxiv.org/abs/2111.12792)]

**Deep Animation Video Interpolation in the Wild** \
*Li Siyao, Shiyu Zhao, Weijiang Yu, Wenxiu Sun, Dimitris N. Metaxas, Chen Change Loy, Ziwei Liu*\
[6 Apr., 2021] [arXiv, 2021] \
[[paper](https://arxiv.org/abs/2104.02495)] | [[project](https://github.com/lisiyao21/AnimeInterp/)] | [[dataset](https://github.com/lisiyao21/AnimeInterp/)]

**Deep Sketch-guided Cartoon Video Inbetweening** \
*Xiaoyu Li, Bo Zhang, Jing Liao, Pedro V. Sander*\
[10 Aug., 2020] [arXiv, 2020] \
[[paper](https://arxiv.org/abs/2008.04149)] | [webpage] | [demo] | [repo] | [dataset]

**Optical Flow Based Line Drawing Frame Interpolation Using Distance Transform to Support Inbetweenings** \
*Rei Narita, Keigo Hirakawa, Kiyoharu Aizawa*\
[26 Aug., 2019] [IEEE, 2019] \
[[paper](https://ieeexplore.ieee.org/document/8803506)] | [webpage] | [demo] | [repo] | [dataset]

**DiLight: Digital light table ‚Äì Inbetweening for 2D animations using guidelines** \
*Leonardo Carvalho, Ricardo Marroquim, Emilio Vital Brazil*\
[Jun., 2017] [Elsevier, 2017] \
[[paper](https://www.sciencedirect.com/science/article/abs/pii/S0097849317300390)] | [webpage] | [demo] | [repo] | [dataset]


## Editing

**Re:Draw -- Context Aware Translation as a Controllable Method for Artistic Production** \
*Joao Liborio Cardoso, Francesco Banterle, Paolo Cignoni, Michael Wimmer*\
[Jan., 2024] [*TBA 2024] \
[[paper](https://arxiv.org/abs/2401.03499)] | [webpage] | [demo] | [repo] | [dataset]

**Sprite-from-Sprite: Cartoon Animation Decomposition with Self-supervised Sprite Estimation** \
*Lvmin Zhang, Tien-Tsin Wong, Yuxin Liu*\
[Nov., 2022] [ACM 2022] \
[[paper](https://dl.acm.org/doi/pdf/10.1145/3550454.3555439)] | [webpage] | [demo] | [[repo](https://lllyasviel.github.io/GitPageToonDecompose/)] | [dataset]

**Toonsynth: example-based synthesis of hand-colored cartoon animations** \
*M Dvoro≈æn√°k, W Li, VG Kim, D S√Ωkora*\
[Jul., 2018] [TOG 2018] \
[[paper](https://dl.acm.org/doi/abs/10.1145/3197517.3201326)] | [webpage] | [demo] | [repo] | [dataset]


## Tracking & Matching

**Globally Optimal Toon Tracking** \
*Haichao Zhu, Xueting Liu, Tien-Tsin Wong, Pheng-Ann Heng* \
[11 Jul., 2016] [TOG, 2016] \
[[paper](https://dl.acm.org/doi/10.1145/2897824.2925872)] | [webpage] | [demo] | [repo] | [dataset]

## Segmentation

**Stereoscopizing Cel Animations** \
*Xueting Liu, Xiangyu Mao, Xuan Yang, Linling Zhang, Tien-Tsin Wong* \
[11 Jul., 2016] [ACM, 2013] \
[[paper](https://dl.acm.org/doi/abs/10.1145/2508363.2508396)] | [webpage] | [demo] | [repo] | [dataset]


## How to Contribute
We encourage animation enthusiasts, researchers, and scholars to contribute to this repository by adding relevant papers, articles, and resources. Your contributions will help build a valuable reference for anyone interested in the art and science of animation.

To contribute, simply fork this repository, make your additions or improvements, and submit a pull request. Please follow the contribution guidelines outlined in the repository's README file.
